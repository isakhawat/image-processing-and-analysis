{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN with TensorFlow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs4xVYvpZkRl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# load data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEaiFwYGZei9",
        "colab_type": "code",
        "outputId": "73bb5500-ed7b-4f2d-9f3a-7f276c7f10f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  # শুধুমাত্র কোলাবে চেষ্টা করবো টেন্সর-ফ্লো ২.০ এর জন্য\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd3MnYbxZyYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM52QD8WaI6z",
        "colab_type": "text"
      },
      "source": [
        "** import the data!**\n",
        "\n",
        "**one_hot=True argument** it converts the categorical class labels to binary vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbMhauLNat1u",
        "colab_type": "code",
        "outputId": "0ce6b142-0155-413b-fc93-12bed55cc54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = input_data.read_data_sets('data/fashion',one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
            "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTZXVW4pa6LF",
        "colab_type": "text"
      },
      "source": [
        "# Analyze the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jR7Cnpa9WN",
        "colab_type": "code",
        "outputId": "4d486fff-3f3d-41d1-c9e7-55b2dd0930c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Shapes of training set\n",
        "print(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\n",
        "print(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n",
        "\n",
        "# Shapes of test set\n",
        "print(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\n",
        "print(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (images) shape: (55000, 784)\n",
            "Training set (labels) shape: (55000, 10)\n",
            "Test set (images) shape: (10000, 784)\n",
            "Test set (labels) shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WCYVJSIbLNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary of target classes\n",
        "label_dict = {\n",
        " 0: 'T-shirt/top',\n",
        " 1: 'Trouser',\n",
        " 2: 'Pullover',\n",
        " 3: 'Dress',\n",
        " 4: 'Coat',\n",
        " 5: 'Sandal',\n",
        " 6: 'Shirt',\n",
        " 7: 'Sneaker',\n",
        " 8: 'Bag',\n",
        " 9: 'Ankle boot',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlE4ZoxvbW8r",
        "colab_type": "code",
        "outputId": "ae2c465a-17da-4ac8-d381-5cf887d92ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "curr_img = np.reshape(data.train.images[0], (28,28))\n",
        "curr_lbl = np.argmax(data.train.labels[0,:])\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "curr_img = np.reshape(data.test.images[0], (28,28))\n",
        "curr_lbl = np.argmax(data.test.labels[0,:])\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '(Label: Sneaker)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAACuCAYAAABN9Xq+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io_O09MBcP-L",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "The images are of size 28 x 28 (or a 784-dimensional vector)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ75lKbhb4m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " data.train.images[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVO-882oclNE",
        "colab_type": "code",
        "outputId": "4fa74e04-f475-4d56-8165-6254fd340be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.max(data.train.images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9960785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNbf5MgPclh6",
        "colab_type": "code",
        "outputId": "e391f19a-b1dd-40f2-e8d0-61d4961ee60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.min(data.train.images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvP9F_ycwwo",
        "colab_type": "text"
      },
      "source": [
        "**Let us reshape the images so that it's of size 28 x 28 x 1, and feed this as an input to the network.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWnia5E1czFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape training and testing image\n",
        "train_X = data.train.images.reshape(-1, 28, 28, 1)\n",
        "test_X = data.test.images.reshape(-1,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruKP9sb-czXt",
        "colab_type": "code",
        "outputId": "d6d9777e-dee9-47cc-b3df-bac0106b3766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_X.shape, test_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55000, 28, 28, 1), (10000, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt8Y8_48dCPN",
        "colab_type": "text"
      },
      "source": [
        "**training and testing labels in separate variables and also print their respective shapes just be on the safer side.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnMFgDG5dFCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = data.train.labels\n",
        "test_y = data.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVjuiCi_cpAo",
        "colab_type": "code",
        "outputId": "357484a6-52db-438d-b942-682d276ef4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_y.shape, test_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55000, 10), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bbc7DjPdVnS",
        "colab_type": "text"
      },
      "source": [
        "# The Deep Neural Network\n",
        "You'll use three convolutional layers:\n",
        "\n",
        "The first layer will have 32-3 x 3 filters,\n",
        "\n",
        "The second layer will have 64-3 x 3 filters and\n",
        "\n",
        "The third layer will have 128-3 x 3 filters.\n",
        "\n",
        "In addition, there are three max-pooling layers each of size 2 x 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GoRLGr9dYQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_iters = 200 \n",
        "learning_rate = 0.001 \n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3tPEHpkdu3S",
        "colab_type": "text"
      },
      "source": [
        "# Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMPllGopdpDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST data input (img shape: 28*28)\n",
        "n_input = 28\n",
        "\n",
        "# MNIST total classes (0-9 digits)\n",
        "n_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ma6agfYd8em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#both placeholders are of type float\n",
        "x = tf.placeholder(\"float\", [None, 28,28,1])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E8lVTDGeHoq",
        "colab_type": "text"
      },
      "source": [
        "**Creating wrappers for simplicity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc8w8Pi0eAaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x) \n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGL8QNNeAmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {\n",
        "    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "}\n",
        "biases = {\n",
        "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztFZY0mfekju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_net(x, weights, biases):  \n",
        "\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n",
        "    conv3 = maxpool2d(conv3, k=2)\n",
        "\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Output, class prediction\n",
        "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ4MmdlheGcl",
        "colab_type": "text"
      },
      "source": [
        "# Loss and Optimizer Nodes\n",
        "\n",
        "**conv_net()** function by passing in input **x**, **weights** and **biases**\n",
        "\n",
        "label **y**. You will then take the mean (**reduce_mean**) over all the batches to get a **single loss/cost value.**\n",
        "\n",
        "**optimizer** specify the learning rate with explicitly stating **minimize cost **that you had **calculated** in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyaLfxcdekxb",
        "colab_type": "code",
        "outputId": "2da5e819-c392-49c7-f703-2e8a49bc4d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "pred = conv_net(x, weights, biases)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-37-989f812044df>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fTUlcr0hgx1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Model Node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6v199hNhq1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "\n",
        "#calculate accuracy across all the given images and average them out. \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCIVxheWhqyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaMpaIEh0Qb",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntOttN68h2H4",
        "colab_type": "code",
        "outputId": "04dfde8d-d2ab-4b1d-98af-eb14307b8d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init) \n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "    for i in range(training_iters):\n",
        "        for batch in range(len(train_X)//batch_size):\n",
        "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
        "            # Run optimization op (backprop).\n",
        "                # Calculate batch loss and accuracy\n",
        "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
        "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                      \"{:.5f}\".format(acc))\n",
        "        print(\"Optimization Finished!\")\n",
        "\n",
        "        # Calculate accuracy for all 10000 mnist test images\n",
        "        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
        "        train_loss.append(loss)\n",
        "        test_loss.append(valid_loss)\n",
        "        train_accuracy.append(acc)\n",
        "        test_accuracy.append(test_acc)\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "    summary_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Loss= 0.022127, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.96520\n",
            "Iter 1, Loss= 0.004514, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98540\n",
            "Iter 2, Loss= 0.004382, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98670\n",
            "Iter 3, Loss= 0.002093, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98810\n",
            "Iter 4, Loss= 0.001003, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98750\n",
            "Iter 5, Loss= 0.000890, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98710\n",
            "Iter 6, Loss= 0.001663, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98700\n",
            "Iter 7, Loss= 0.010365, Training Accuracy= 0.99219\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98720\n",
            "Iter 8, Loss= 0.000306, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98670\n",
            "Iter 9, Loss= 0.000610, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98740\n",
            "Iter 10, Loss= 0.003540, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98580\n",
            "Iter 11, Loss= 0.000061, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98810\n",
            "Iter 12, Loss= 0.000209, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98640\n",
            "Iter 13, Loss= 0.000017, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99170\n",
            "Iter 14, Loss= 0.000002, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99010\n",
            "Iter 15, Loss= 0.000279, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99040\n",
            "Iter 16, Loss= 0.000027, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99070\n",
            "Iter 17, Loss= 0.000045, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99140\n",
            "Iter 18, Loss= 0.000497, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99000\n",
            "Iter 19, Loss= 0.000109, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98840\n",
            "Iter 20, Loss= 0.012261, Training Accuracy= 0.99219\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98790\n",
            "Iter 21, Loss= 0.000028, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99140\n",
            "Iter 22, Loss= 0.000008, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99060\n",
            "Iter 23, Loss= 0.000027, Training Accuracy= 1.00000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.99170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2q5UaEFh2dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
        "plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
        "plt.title('Training and Test loss')\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPyUvem1iO6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9zPvgf1iO12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}